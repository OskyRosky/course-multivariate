Generate sample from N(mu, Sigma)
bivn <- mvrnorm(5000, mu = mu, Sigma = Sigma )  # from Mass package
head(bivn)
# Calculate kernel density estimate
bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)   # from MASS package
#R offers several ways of visualizing the distribution. These next two lines of
#code overlay a contour plot on a "heat Map" that maps
# Contour plot overlayed on heat map image of results
image(bivn.kde)       # from base graphics package
contour(bivn.kde, add = TRUE)
library(hexbin)
x <- rnorm(1000)
y <- rnorm(1000)
bin<-hexbin(x, y, xbins=50)
plot(bin, main="Hexagonal Binning")
library(lattice)
attach(mtcars)
# create factors with value labels
gear.f<-factor(gear,levels=c(3,4,5),
labels=c("3gears","4gears","5gears"))
cyl.f <-factor(cyl,levels=c(4,6,8),
labels=c("4cyl","6cyl","8cyl"))
# kernel density plot
densityplot(~mpg,
main="Density Plot",
xlab="Miles per Gallon")
# kernel density plots by factor level
densityplot(~mpg|cyl.f,
main="Density Plot by Number of Cylinders",
xlab="Miles per Gallon")
# kernel density plots by factor level
densityplot(~mpg|cyl.f,
main="Density Plot by Number of Cylinders",
xlab="Miles per Gallon")
# kernel density plots by factor level (alternate layout)
densityplot(~mpg|cyl.f,
main="Density Plot by Numer of Cylinders",
xlab="Miles per Gallon",
layout=c(1,3))
library(ellipse)
rho <- cor(bivn)
y_on_x <- lm(bivn[,2] ~ bivn[,1])    # Regressiion Y ~ X
x_on_y <- lm(bivn[,1] ~ bivn[,2])    # Regression X ~ Y
plot_legend <- c("99% CI green", "95% CI red","90% CI blue",
"Y on X black", "X on Y brown")
plot(bivn, xlab = "X", ylab = "Y",
col = "dark blue",
main = "Bivariate Normal with Confidence Intervals")
lines(ellipse(rho), col="red")       # ellipse() from ellipse package
lines(ellipse(rho, level = .99), col="green")
lines(ellipse(rho, level = .90), col="blue")
abline(y_on_x)
abline(x_on_y, col="brown")
legend(3,1,legend=plot_legend,cex = .5, bty = "n")
The next bit of code generates a couple of three dimensional surface plots. The second of which is an rgl plot that you will be able to rotate and view from different perspectives on your screen.
# Three dimensional surface
# Basic perspective plot
persp(bivn.kde, phi = 45, theta = 30, shade = .1, border = NA) # from base graphics package
library(lattice)
attach(mtcars)
data(mtcars)
# create factors with value labels
gear.f<-factor(gear,levels=c(3,4,5),
labels=c("3gears","4gears","5gears"))
# boxplots for each combination of two factors
bwplot(cyl.f~mpg|gear.f,
ylab="Cylinders", xlab="Miles per Gallon",
main="Mileage by Cylinders and Gears",
layout=(c(1,3)))
library(vcd)
mosaic(HairEyeColor, shade=TRUE, legend=TRUE)
assoc(HairEyeColor, shade=TRUE)
library(vcd)
mosaic(HairEyeColor, shade=TRUE, legend=TRUE)
#The first part of this article describes quickly how to compute and visualize principal component analysis
#using FactoMineR and factoextra
#The second part shows how to identify the most important
#variables that explain the variations in your data
#Librerías#
install.packages("FactoMineR")
library(FactoMineR)
library(factoextra)
install.packages("FactoMineR")
install.packages("factoextra")
install.packages("devtools")
install.packages("ggplot2")
library(factoextra)
install.packages("factoextra")
library(factoextra)
data(decathlon2)
install.packages("FactoMineR")
library(FactoMineR)
install.packages("FactorMineR")
library(FactoMineR)
install.packages("FactoMineR")
library(FactoMineR)
library(FactoMineR)
install.packages("FactoMiner")
#############################################
#                                           #
#            ANÁLISIS DISCRIMINANTE         #
#                                           #
#############################################
#####################
# Ejemplo 1       #
######################
#En este primer caso de estudio, el conjunto de datos del vino, tenemos 13 concentraciones
#químicas que describen muestras de vino de tres cultivares.
install.packages("car")
install.packages("rattle")
install.packages("klaR")
library(car)
data(wine, package='rattle')
attach(wine)
head(wine)
library(car)
library(rattle)
data(wine, package='rattle')
attach(wine)
head(wine)
UCI <- "http://archive.ics.uci.edu/ml"
REPOS <- "machine-learning-databases"
wine.url <- sprintf("
wine <- read.csv(wine.url, header=FALSE)
colnames(wine) <- c('Type', 'Alcohol', 'Malic', 'Ash',
'Alcalinity', 'Magnesium', 'Phenols',
'Flavanoids', 'Nonflavanoids',
'Proanthocyanins', 'Color', 'Hue',
'Dilution', 'Proline')
wine$Type <- as.factor(wine$Type)
save(wine, file="wine.Rdata", compress=TRUE)
install.packages("rattle")
install.packages("car.data")
library(rattle.data)
install.packages("car.data")
install.packages("rattle.data")
library(rattle.data)
data(wine, package='rattle.data')
attach(wine)
head(wine)
library(rattle.data)
data(wine, package='rattle.data')
attach(wine)
head(wine)
wine
scatterplotMatrix(wine[2:6])
library(car)
scatterplotMatrix(wine[2:6])
scatterplotMatrix(wine)
library(MASS)
wine.lda <- lda(Type ~ ., data=wine)
names(wine.lda)
wine.lda
wine.lda.values <- predict(wine.lda)
ldahist(data = wine.lda.values$x[,1], g=Type)
ldahist(data = wine.lda.values$x[,2], g=Type)
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
#Primera función discriminante
wine.lda.values <- predict(wine.lda)
ldahist(data = wine.lda.values$x[,1], g=Type)
gunda función discriminante
ldahist(data = wine.lda.values$x[,2], g=Type)
plot(wine.lda.values$x[,1],wine.lda.values$x[,2]) #
text(wine.lda.values$x[,1],wine.lda.values$x[,2],Type,cex=0.7,pos=4,col="red")
url <- 'http://www.biz.uiowa.edu/faculty/jledolter/DataMining/admission.csv'
admit <- read.csv(url)
head(admit)
admit
attach(admit)
## Apliquemos la función de discriminación lineal
m1=lda(De~.,admit)
m1
predict(m1,newdata=data.frame(GPA=3.21,GMAT=497))
predict(m1,newdata=data.frame(GPA=3.21,GMAT=497))$class
predict(m1,newdata=data.frame(GPA=1.21,GMAT=307))$class
m2=qda(De~.,admit)
m2
m2=qda(De~.,admit)
m2
predict(m2,newdata=data.frame(GPA=3.21,GMAT=497))
n=85
nt=60
neval=n-nt
rep=100
### LDA
set.seed(123456789)
errlin=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## linear discriminant analysis
m1=lda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errlin[k]=(neval-sum(diag(tablin)))/neval
}
merrlin=mean(errlin)
merrlin
### Modelo cuadrático
set.seed(123456789)
errqda=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## quadratic discriminant analysis
m1=qda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errqda[k]=(neval-sum(diag(tablin)))/neval
}
merrqda=mean(errlin)
merrqda
n=85
nt=60
neval=n-nt
rep=100
### LDA
set.seed(123456789)
errlin=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## linear discriminant analysis
m1=lda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errlin[k]=(neval-sum(diag(tablin)))/neval
}
merrlin=mean(errlin)
merrlin
### Modelo cuadrático
set.seed(123456789)
errqda=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## quadratic discriminant analysis
m1=qda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errqda[k]=(neval-sum(diag(tablin)))/neval
}
merrqda=mean(errlin)
merrqda
#Alcanzamos una tasa de error de 10,2% en ambos casos. R también nos dan algunas
#herramientas de visualización. Por ejemplo biblioteca klaR:
# Explorando los gráficos  Graph for LDA or QDA
#install.packages('klaR')
library(klaR)
partimat(De~.,data=adm,method="lda")
adm<-admit
adm<-admit
n=85
nt=60
neval=n-nt
rep=100
### LDA
set.seed(123456789)
errlin=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## linear discriminant analysis
m1=lda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errlin[k]=(neval-sum(diag(tablin)))/neval
}
merrlin=mean(errlin)
merrlin
### Modelo cuadrático
set.seed(123456789)
errqda=dim(rep)
for (k in 1:rep) {
train=sample(1:n,nt)
## quadratic discriminant analysis
m1=qda(De~.,adm[train,])
predict(m1,adm[-train,])$class
tablin=table(adm$De[-train],predict(m1,adm[-train,])$class)
errqda[k]=(neval-sum(diag(tablin)))/neval
}
merrqda=mean(errlin)
merrqda
merrlin;merrqda
BC_1 =1-merrlin
BC_2 =1-merrqda
BC_1 ; BC_2
library(klaR)
partimat(De~.,data=adm,method="lda")
## Datos
credit <- read.csv("http://www.biz.uiowa.edu/faculty/jledolter/DataMining/germancredit.csv")
head(credit,2)
# Como puede verse, sólo las variables: duración, cantidad, cuota y edad son numéricas.
cred1=credit[, c("Default","duration","amount","installment","age")]
head(cred1)
#Algunas estadísticas descriptivas
summary(cred1)
#interesa verificar la normalidad de las variables
hist(cred1$duration)
hist(cred1$amount)
hist(cred1$installment)
hist(cred1$age)
# creando un data.frame
cred1=data.frame(cred1)
#Apliquemos la función discriminante
library(MASS)
attach(cred1)
## LDA: class proportions of the training set used as prior probabilities
zlin=lda(Default~.,cred1)
table(predict(zlin)$class, Default)
install.packages("rattle")
install.packages("cluster")
install.packages("reshape")
data(wine, package='rattle')
head(wine)
install.packages("rattle.data")
data(wine, package='rattle.data')
head(wine)
View(wine)
head(wine)
wine.stand <- scale(wine[-1])  # con scale hacemos la estandarización
k.means.fit <- kmeans(wine.stand, 3) # fijamos en k = 3
k.means.fit
attributes(k.means.fit)
k.means.fit$centers
k.means.fit$cluster
k.means.fit$size
wssplot <- function(data, nc=15, seed=1234){
wss <- (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:nc){
set.seed(seed)
wss[i] <- sum(kmeans(data, centers=i)$withinss)}
plot(1:nc, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")}
wssplot(wine.stand, nc=6)
wssplot(wine.stand, nc=10)
wssplot(wine.stand, nc=15)
wssplot(wine.stand, nc=30)
wssplot(wine.stand, nc=3000)
wssplot(wine.stand, nc=100)
# Mediante la librería cluster podemos representar la solución  en 2 dimensiones
library(cluster)
clusplot(wine.stand, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
?kmeans
library(cluster)
clusplot(wine.stand, k.means.fit$cluster, main='2D representation of the Cluster solution',
color=TRUE, shade=TRUE,
labels=2, lines=0)
table(wine[,1],k.means.fit$cluster)
d <- dist(wine.stand, method = "euclidean") # Matriz de distancias euclidiana
d
H.fit <- hclust(d, method="ward")
H.fit <- hclust(d, method="ward.D2")
plot(H.fit) # Mostrar dendrograma
# Dibujar dendrograma con bordes rojos alrededor de los 5 grupos
rect.hclust(H.fit, k=3, border="red")
table(wine[,1],groups)
groups <- table(wine[,1],k.means.fit$cluster)
table(wine[,1],groups)
plot(H.fit) # Mostrar dendrograma
groups <- cutree(H.fit, k=3) # Corte el árbol en 5 grupos
# Dibujar dendrograma con bordes rojos alrededor de los 5 grupos
rect.hclust(H.fit, k=3, border="red")
table(wine[,1],groups)
install.packages("rpart")
install.packages("party")
# Classification Tree with rpart
library(rpart)
data(kyphosis)
head(kyphosis)
fit <- rpart(Kyphosis ~ Age + Number + Start,
method="class", data=kyphosis)
names(fit)
fit$method
printcp(fit) # muestra los resultdos
plotcp(fit) # visualize cross-validation results
summary(fit) # El resumen en lo más importante
predicted= predict(fit)
predicted
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
summary(pfit)
# plot the pruned tree
plot(pfit, uniform=TRUE,  main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
# Regression Tree Example
library(rpart)
data(cu.summary)
head(cu.summary)
# Hagamos crecer el árbol
fit <- rpart(Mileage~Price + Country + Reliability + Type,
method="anova", data=cu.summary)   #OJO que en method cambiamos a ANOVA
summary(fit)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
predicted= predict(fit)
predicted
# create additional plots
par(mfrow=c(1,2)) # two plots on one page
rsq.rpart(fit) # visualize cross-validation results
# plot tree
plot(fit, uniform=TRUE,
main="Regression Tree for Mileage ")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
#######################
#  La poda del árbol  #
#######################
pfit<- prune(fit, cp=0.01160389) # from cptable
# plot the pruned tree
plot(pfit, uniform=TRUE,
main="Pruned Regression Tree for Mileage")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
predicted2= predict(pfit)
predicted2
library("party")
str(iris)
iris_ctree <- ctree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
print(iris_ctree)
plot(iris_ctree)
plot(iris_ctree, type="simple")
install.packages("rpart")
install.packages("party")
library(rpart)
data(kyphosis)
head(kyphosis)
# Crecimiento del árbol
fit <- rpart(Kyphosis ~ Age + Number + Start,
method="class", data=kyphosis)
names(fit)
fit$method
printcp(fit) # muestra los resultdos
plotcp(fit) # visualize cross-validation results
summary(fit) # El resumen en lo más importante
predicted= predict(fit)
predicted
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# prune the tree
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
summary(pfit)
# plot the pruned tree
plot(pfit, uniform=TRUE,  main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
predicted= predict(fit)
predicted
# Gráfico del árbol
#Es mejor visualizar los árboles mediante un gráfico. Todo esto a continuación.
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
#La poda del árbol
#Vamos a evitar la sobre o sub clasificación
# prune the tree
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
summary(pfit)
# plot the pruned tree
plot(pfit, uniform=TRUE,  main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
names(fit)
fit$method
printcp(fit) # muestra los resultdos
plotcp(fit) # visualize cross-validation results
summary(fit) # El resumen en lo más importante
predicted= predict(fit)
predicted
# Gráfico del árbol
#Es mejor visualizar los árboles mediante un gráfico. Todo esto a continuación.
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
# prune the tree
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
summary(pfit)
# plot the pruned tree
plot(pfit, uniform=TRUE,  main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
plot(fit, uniform=TRUE,
main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
pfit<- prune(fit, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
summary(pfit)
# plot the pruned tree
plot(pfit, uniform=TRUE,  main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
library("party")
str(iris)
iris_ctree <- ctree(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=iris)
print(iris_ctree)
plot(iris_ctree)
plot(iris_ctree, type="simple")
